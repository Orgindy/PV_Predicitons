# PV Predictions

This repository contains a collection of scripts to process climate data and evaluate radiative cooling (RC) technologies combined with photovoltaic (PV) systems.

## Features
- ERA5 aggregation and preprocessing
- Clustering workflows for RC/PV potential
- Thermal modeling utilities
- Streamlit dashboard (`app.py`)

## System-Level Prerequisites

Some helper libraries must be available before Python packages are installed.
Install the development headers for GDAL, GEOS, PROJ and ecCodes.

On Ubuntu:

```bash
sudo apt-get update
sudo apt-get install libgdal-dev libgeos-dev libproj-dev libeccodes-dev
```

On macOS:

```bash
brew install gdal geos proj eccodes
```

Install these packages **before** running `scripts/setup_env.sh` to set up the
Python environment.

## Requirements
Install dependencies from `requirements.txt` before running any of the scripts or tests.
The project is tested on **Python 3.11**.
You can do this manually or via the helper script in `scripts/setup_env.sh`:

```bash
# optional: create a virtual environment
python3.11 -m venv venv && source venv/bin/activate

# install packages manually
pip install -r requirements.txt

# or use the setup script
bash scripts/setup_env.sh

# optional: verify all imports succeed
python scripts/check_imports.py
# or check every file individually
python scripts/check_all_imports.py
```

## NetCDF data directory

Many scripts read NetCDF files from a shared location. Set the directory
via the `NC_DATA_DIR` environment variable or create a `config.yaml` file
with a key `nc_data_dir`. An example file is provided:

```bash
cp config.yaml.example config.yaml
echo "nc_data_dir: /path/to/my/netcdf" > config.yaml
# Example for Windows users
# echo "nc_data_dir: C:\Users\gindi002\DATASET\Era5_GRIB_NEW_dataset_2023-2019_and_more_future" > config.yaml
# (This Windows path is also included as a comment in config.yaml.example.)
```

The environment variable takes precedence. `config.yaml` is ignored by Git so you
can adjust it locally. Individual scripts still
allow `--netcdf-file` arguments to override this location for a single run.

## Input files

Several CSV and NetCDF files are expected by the pipeline. They are typically
generated by earlier preprocessing steps and should be placed in the project
root or inside a `data/` directory.

- `merged_dataset.csv` – combined ERA5 and remote sensing parameters read by
  `feature_preparation.py`.
- `validated_dataset.csv` – cleaned output from `feature_preparation.py`.
- `physics_dataset.csv` – dataset with physics-based PV potential.
- `clustered_dataset.csv` – produced by `spatial_mapping.py` and used as the
  default input for `main.py`.
- `ERA5_daily.nc` – processed ERA5 climate data. Public downloads are available
  from the [Copernicus Climate Data Store](https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels).

NetCDF files should reside under the directory configured with `NC_DATA_DIR` or
`config.yaml`.

### Quick start with sample data

Minimal CSV examples are provided under `tests/data/` for experimentation. Copy
them to the project root to run the scripts without downloading full datasets:

```bash
cp tests/data/sample_clustered_dataset.csv clustered_dataset.csv
python synergy_index.py --input clustered_dataset.csv --output results/sample_synergy.csv

cp tests/data/sample_merged_dataset.csv merged_dataset.csv
python feature_preparation.py --input-file merged_dataset.csv \
    --validated-file validated_dataset.csv --physics-file physics_dataset.csv
```

These tiny files contain only a couple of rows so the commands complete very quickly.

## Running Tests
Unit tests rely on the packages listed in `requirements.txt`. **Always install
these dependencies before running `pytest`.** You can install them manually or
use the helper script:

```bash
pip install -r requirements.txt
# or
bash scripts/setup_env.sh

# optional sanity check for missing packages
python scripts/check_imports.py

pytest
```


## Database Integration

Several scripts can read from or write to a SQL database using SQLAlchemy. Set
the `PV_DB_URL` environment variable or use the `--db-url` and `--db-table`
options to enable this feature.

Example using SQLite:

```bash
export PV_DB_URL=sqlite:///path/to/pv.sqlite
python feature_preparation.py --db-url $PV_DB_URL --db-table raw_pv_data
```

### Local database setup

If your database file is on your local drive, provide the absolute path with the
`sqlite:///` scheme. The same URL can be used with `main.py` or any script that
accepts the `--db-url` option:

```bash
export PV_DB_URL="sqlite:////full/path/to/pv.sqlite"
python main.py --db-url $PV_DB_URL --db-table pv_data

To quickly verify the connection works, run `check_db_connection.py`:
```bash
python check_db_connection.py --db-url $PV_DB_URL
```

## Usage

Several scripts accept file paths via command line arguments. Get help with:

```bash
python feature_preparation.py --help
python pv_prediction.py --help
python spatial_mapping.py --help
python metadata_inspection.py --help
```

### Feature Preparation

```bash
python feature_preparation.py \
  --input-file data/merged_dataset.csv \
  --validated-file data/validated_dataset.csv \
  --physics-file data/physics_dataset.csv \
  # Optional: override the NetCDF directory
  --netcdf-file my_dataset.nc \
  --results-dir results
```

### SMARTS Batch Processing

```bash
python run_smarts_batch.py \
  --smarts-exe /path/to/smarts295bat.exe \
  --inp-dir smarts_inp_files \
  --out-dir smarts_out_files
```

### Streamlit Dashboard

```bash
streamlit run app.py -- --data-path matched_dataset.csv
```

The dashboard can also load data directly from a database:

```bash
streamlit run app.py -- --db-url sqlite:///pv.sqlite --db-table pv_results
```

### Multi-Year PV Technology Matching

Run the clustering and technology matching pipeline for several yearly datasets.
The script looks for `clustered_dataset_<year>.csv` files in a given directory
and produces technology-matched outputs for each year along with a combined
file.

```bash
python multi_year_controller.py
```

Each CLI script that reads or writes CSVs supports database options. Examples:

```bash
python synergy_index.py --db-url sqlite:///pv.sqlite --db-table pv_data
python clustering.py --db-url sqlite:///pv.sqlite --db-table clustered
python pv_prediction.py --db-url sqlite:///pv.sqlite --db-table pv_results
python rc_climate_zoning.py --db-url sqlite:///pv.sqlite --db-table zones
python multi_year_controller.py --db-url sqlite:///pv.sqlite --db-table results
```


## Utility Examples

Use `ResourceMonitor` to verify system limits before running heavy tasks:

```python
from utils.resource_monitor import ResourceMonitor
print(ResourceMonitor.check_system_resources())
```

Write files atomically using `SafeFileOps`:

```python
from utils.file_operations import SafeFileOps
from pathlib import Path

SafeFileOps.atomic_write(Path("example.txt"), "hello world")
```

Remove temporary files automatically:

```python
from utils.resource_monitor import ResourceCleanup

with ResourceCleanup.cleanup_context():
    # do work that creates *.tmp files
    pass
```

