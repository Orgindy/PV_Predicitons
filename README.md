# PV Predictions

This repository contains a collection of scripts to process climate data and evaluate radiative cooling (RC) technologies combined with photovoltaic (PV) systems.

## Features
- ERA5 aggregation and preprocessing
- Clustering workflows for RC/PV potential
- Thermal modeling utilities
- Streamlit dashboard (`app.py`)

## System-Level Prerequisites

Some helper libraries must be available before Python packages are installed.
Install the development headers for GDAL, GEOS, PROJ and ecCodes.

On Ubuntu:

```bash
sudo apt-get update
sudo apt-get install libgdal-dev libgeos-dev libproj-dev libeccodes-dev
```

On macOS:

```bash
brew install gdal geos proj eccodes
```

Install these packages **before** running `scripts/setup_env.sh` to set up the
Python environment.

## Requirements
Install dependencies from `requirements.txt` before running any of the scripts or tests.
The project is tested on **Python 3.11**.
You can do this manually or via the helper script in `scripts/setup_env.sh`:

```bash
# optional: create a virtual environment
python3.11 -m venv venv && source venv/bin/activate

# install packages manually
pip install -r requirements.txt

# or use the setup script
bash scripts/setup_env.sh

# optional: verify all imports succeed
python scripts/check_imports.py
# or check every file individually
python scripts/check_all_imports.py
```

## NetCDF data directory

Many scripts read NetCDF files from a shared location. Configure this location
either in `config.yaml` or with the `NC_DATA_DIR` environment variable. Copy the
template and edit it:

```bash
cp config.yaml.example config.yaml
```

Example `config.yaml`:

```yaml
# config.yaml
nc_data_dir: /path/to/my/netcdf
```

Or set the path via an environment variable:

```bash
export NC_DATA_DIR=/path/to/my/netcdf
```

`NC_DATA_DIR` overrides the value in `config.yaml`. Individual scripts can also
pass `--netcdf-file` to override both for a single run. Database connections are
configured with the `PV_DB_URL` environment variable.

## Input files

Several CSV and NetCDF files are expected by the pipeline. They are typically
generated by earlier preprocessing steps and should be placed in the project
root or inside a `data/` directory.

- `merged_dataset.csv` – combined ERA5 and remote sensing parameters read by
  `Feature Preparation.py`.
- `validated_dataset.csv` – cleaned output from `Feature Preparation.py`.
- `physics_dataset.csv` – dataset with physics-based PV potential.
- `clustered_dataset.csv` – produced by `Spatial Mapping.py` and used as the
  default input for `main.py`.
- `ERA5_daily.nc` – processed ERA5 climate data. Public downloads are available
  from the [Copernicus Climate Data Store](https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels).

NetCDF files should reside under the directory configured with `NC_DATA_DIR` or
`config.yaml`.

## Running Tests
Unit tests use `pytest`. Install the dependencies first (for example by running
`bash scripts/setup_env.sh` as shown above), then execute:

```bash
pytest -q
```

## Database Integration

Several scripts can read from or write to a SQL database using SQLAlchemy. Set
the `PV_DB_URL` environment variable or use the `--db-url` and `--db-table`
options to enable this feature.

Example using SQLite:

```bash
export PV_DB_URL=sqlite:///path/to/pv.sqlite
python "Feature Preparation.py" --db-url $PV_DB_URL --db-table raw_pv_data
```

### Local database setup

If your database file is on your local drive, provide the absolute path with the
`sqlite:///` scheme. The same URL can be used with `main.py` or any script that
accepts the `--db-url` option:

```bash
export PV_DB_URL="sqlite:////full/path/to/pv.sqlite"
python main.py --db-url $PV_DB_URL --db-table pv_data

To quickly verify the connection works, run `check_db_connection.py`:
```bash
python check_db_connection.py --db-url $PV_DB_URL
```

## Usage

Several scripts now accept file paths via command line arguments:

**Note on quoting script names**
Scripts whose filenames contain spaces must be wrapped in quotes when executed.
Examples:

```bash
python "Feature Preparation.py" --help
python "PV prediction.py" --help
python "Spatial Mapping.py" --help
python "Metadata Inspection.py" --help
```

### Feature Preparation

```bash
python "Feature Preparation.py" \
  --input-file data/merged_dataset.csv \
  --validated-file data/validated_dataset.csv \
  --physics-file data/physics_dataset.csv \
  # Optional: override the NetCDF directory
  --netcdf-file my_dataset.nc \
  --results-dir results
```

### SMARTS Batch Processing

```bash
python run_smarts_batch.py \
  --smarts-exe /path/to/smarts295bat.exe \
  --inp-dir smarts_inp_files \
  --out-dir smarts_out_files
```

### Streamlit Dashboard

```bash
streamlit run app.py -- --data-path matched_dataset.csv
```

The dashboard can also load data directly from a database:

```bash
streamlit run app.py -- --db-url sqlite:///pv.sqlite --db-table pv_results
```

### Multi-Year PV Technology Matching

Run the clustering and technology matching pipeline for several yearly datasets.
The script looks for `clustered_dataset_<year>.csv` files in a given directory
and produces technology-matched outputs for each year along with a combined
file.

```bash
python multi_year_controller.py
```

Each CLI script that reads or writes CSVs supports database options. Examples:

```bash
python synergy_index.py --db-url sqlite:///pv.sqlite --db-table pv_data
python clustering.py --db-url sqlite:///pv.sqlite --db-table clustered
python "PV prediction.py" --db-url sqlite:///pv.sqlite --db-table pv_results
python rc_climate_zoning.py --db-url sqlite:///pv.sqlite --db-table zones
python multi_year_controller.py --db-url sqlite:///pv.sqlite --db-table results
```

